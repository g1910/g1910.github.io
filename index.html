<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DPK93Q2Q3D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DPK93Q2Q3D');
  </script>

  <title>Gaurav Mittal</title>

  <meta name="author" content="Gaurav Mittal">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Gaurav Mittal</name>
              </p>
              <p>I am a Senior Researcher at Microsoft Cloud+AI where I work on the research and development of Computer Vision and Machine Learning related products.
              </p>
              <p>
                I am currently pursuing multi-modal machine learning research as part of <a href="https://azure.microsoft.com/en-us/services/cognitive-services/content-moderator/">Content Moderator under Azure Cognitive Services</a>. Previously, I have worked on AutoML as part of the <a href="https://www.customvision.ai/">Microsoft Custom Vision Service</a>. I did my Master's in Computer Vision at Robotics Institute, Carnegie Mellon University where I worked with Prof. Kris Kitani on model compression and <a href="https://github.com/g1910/HyperNetworks">Hypernetworks</a>. I have also worked at Amazon as a Software Development Engineer. I graduated with a B.Tech. in Computer Science and Engineering from Indian Insitute of Technology Ropar with the President of India Gold Medal. I have won the Microsoft Imagine Cup India and Indian National Academy of Engineering Innovate Student Project Award in 2016 for my project <a href="https://news.microsoft.com/en-in/features/satya-nadella-meets-millennials-empowerindia/">SpotGarbage</a>.
              </p>
              <p style="text-align:center">
                <a href="mailto:gaurav.mittal[at]microsoft.com">Email</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=m-tCelQAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/mitts1910">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/g1910/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/gaurav19m/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/gaurav_mittal.jpeg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/gaurav_mittal.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                <li> <b> July 2022 : </b> Paper accepted at ECCV 2022 (Oral) (details coming soon).
                <li> <b> May 2022 : </b> Outstanding Reviewer for CVPR 2022.
                <li> <b> May 2022 : </b> Reviewer for ECCV 2022.
                <li> <b> Mar 2022 : </b> GateHUB accepted at CVPR 2022.
                <li> <b> Feb 2022 : </b> Patent granted, Patent No.: US 11,238,885 B2.
                <li> <b> Dec 2021 : </b> Reviewer for CVPR 2022.
              <ul>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, multmodal machine learning, automated machine learning (AutoML), semi-/self-supervised learning and meta learning.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/gatehub_demo.gif' width="160"></div>
                <img src='images/gatehub_demo.gif' width="160">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_GateHUB_Gated_History_Unit_With_Background_Suppression_for_Online_Action_CVPR_2022_paper.html">
                <papertitle>GateHUB: Gated History Unit With Background Suppression for Online Action Detection</papertitle>
              </a>
              <br>
              <a href="https://junwenchen.github.io/">Junwen Chen*</a>,
              <strong>Gaurav Mittal*</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=x4IFIuYAAAAJ">Ye Yu</a>,
              <a href="https://people.rit.edu/yukics/">Yu Kong</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Chen_GateHUB_Gated_History_Unit_With_Background_Suppression_for_Online_Action_CVPR_2022_paper.html">paper</a>
              <p></p>
              <p>
                GateHUB introduces a novel gated cross-attention along with future-augmented history and background suppression objective to outperform all existing methods on online action detection task on all public benchmarks.
              </p>
            </td>
          </tr>

          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/metauvfs_iccv.png' width="160"></div>
                <img src='images/metauvfs_iccv.png' width="160">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Kozerawski_BLT_Balancing_Long-Tailed_Datasets_with_Adversarially-Perturbed_Images_ACCV_2020_paper.html">
                <papertitle>Unsupervised Few-Shot Action Recognition via Action-Appearance Aligned Meta-Adaptation</papertitle>
              </a>
              <br>
              <a href="https://web.engr.oregonstate.edu/~patravaj/">Jay Patravali*</a>,
              <strong>Gaurav Mittal*</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=x4IFIuYAAAAJ">Ye Yu</a>,
              <a href="https://web.engr.oregonstate.edu/~lif/">Fuxin Li</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>
              <br>
              <em>ICCV</em>, 2021 <font color="red"><strong>(Oral Presentation, 3% acceptance rate)</strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Patravali_Unsupervised_Few-Shot_Action_Recognition_via_Action-Appearance_Aligned_Meta-Adaptation_ICCV_2021_paper.html">paper</a>
              <p></p>
              <p>
                First Unsupervised Meta-learning algorithm for Video Few-Shot action recognition. It comprises a novel Action-Appearance Aligned Meta-adaptation (A3M) module that learns to focus on the action-oriented video features in relation to the appearance features via explicit few-shot episodic meta-learning over unsupervised hard-mined episodes.
              </p>
            </td>
          </tr>

          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/muse_bmvc.jpeg' width="160"></div>
                <img src='images/muse_bmvc.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.bmvc2021-virtualconference.com/conference/papers/paper_0365.html">
                <papertitle>MUSE: Feature Self-Distillation with Mutual Information and Self-Information</papertitle>
              </a>
              <br>
              <a href="https://yugongg.github.io/">Yu Gong*</a>,
              <a href="https://scholar.google.com/citations?hl=en&user=x4IFIuYAAAAJ">Ye Yu*</a>,
              <strong>Gaurav Mittal</strong>,
              <a href="https://www.cs.sfu.ca/~mori/">Greg Mori</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>
              <br>
              <em>BMVC</em>, 2021
              <br>
              <a href="https://www.bmvc2021-virtualconference.com/assets/papers/0365.pdf">paper</a>
              <p></p>
              <p>
                A novel information-theoretic approach to introduce dependency among features of a deep convolutional neural network (CNN). It jointly improve the expressivity of all features extracted from different layers in a CNN using Additive Information and Multiplicative Information.
              </p>
            </td>
          </tr>

          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/blt_thumb.png' width="160"></div>
                <img src='images/blt_thumb.png' width="160">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Kozerawski_BLT_Balancing_Long-Tailed_Datasets_with_Adversarially-Perturbed_Images_ACCV_2020_paper.html">
                <papertitle>BLT: Balancing Long-Tailed Datasets with Adversarially-Perturbed Images</papertitle>
              </a>
              <br>
              <a href="https://jkozerawski.github.io/">Jedrzej Kozerawski</a>,
              <a href="http://vfragoso.com/">Victor Fragoso</a>,
              <a href="https://karianakis.github.io/">Nikolaos Karianakis</a>,
              <strong>Gaurav Mittal</strong>,
              <a href="https://sites.cs.ucsb.edu/~mturk/">Matthew Turk</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>
              <br>
              <em>ACCV</em>, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Kozerawski_BLT_Balancing_Long-Tailed_Datasets_with_Adversarially-Perturbed_Images_ACCV_2020_paper.pdf">paper</a> /
              <a href="https://github.com/JKozerawski/BLT">code</a> /
              <a href="https://www.youtube.com/watch?v=stMSOwrJToU&feature=youtu.be&ab_channel=J%C4%99drzejKozerawski">video</a>
              <p></p>
              <p>
                A novel data augmentation technique that uses gradient-ascent to generate extra training samples for tail classes in a long-tail class distribution to improve generalization performance of a image classifier for real-world datasets exhibiting long tail. BLT avoids dedicated generative networks for image generation, thereby significantly reducing training time and compute.
              </p>
            </td>
          </tr>


          <tr onmouseout="inerf_stop()" onmouseover="inerf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/hyperstar_thumb.gif' width="160"></div>
                <img src='images/hyperstar_thumb.gif' width="160">
              </div>
              <script type="text/javascript">
                function inerf_start() {
                  document.getElementById('inerf_image').style.opacity = "1";
                }
                function inerf_stop() {
                  document.getElementById('inerf_image').style.opacity = "0";
                }
                inerf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Mittal_HyperSTAR_Task-Aware_Hyperparameters_for_Deep_Networks_CVPR_2020_paper.html">
                <papertitle>HyperSTAR: Task-Aware Hyperparameters for Deep Networks</papertitle>
              </a>
              <br>
              <strong>Gaurav Mittal*</strong>,
              <a href="https://sites.google.com/view/cliu5/home">Chang Liu*</a>,

              <a href="https://karianakis.github.io/">Nikolaos Karianakis</a>,
              <a href="http://vfragoso.com/">Victor Fragoso</a>,
              <a href="https://www.microsoft.com/en-us/research/people/meic/">Mei Chen</a>,
              <a href="http://www1.ece.neu.edu/~yunfu/">Yun Fu</a> (* Equal Contribution)
              <br>
              <em>CVPR</em>, 2020 <font color="red"><strong>(Oral Presentation, 5.7% acceptance rate)</strong></font>
              <br>
              <!-- <a href="http://yenchenlin.me/inerf/">project page</a> / -->
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Mittal_HyperSTAR_Task-Aware_Hyperparameters_for_Deep_Networks_CVPR_2020_paper.html">paper</a> /
              <a href="https://www.youtube.com/watch?v=bvDyoh8vd04&ab_channel=MicrosoftResearch">video</a>
              <p></p>
              <p>A task-aware method to warm-start Hyperparameter Optimization (HPO) methods by predicting the performance for a hyperparamter configuration via a learned task (dataset) representation.
              </p>
            </td>
          </tr>

          <tr onmouseout="flare_stop()" onmouseover="flare_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='talking_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/talking_heads.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/talking_head.png' width="160">
              </div>
              <script type="text/javascript">
                function flare_start() {
                  document.getElementById('talking_image').style.opacity = "0";
                }

                function flare_stop() {
                  document.getElementById('talking_image').style.opacity = "1";
                }
                flare_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Mittal_Animating_Face_using_Disentangled_Audio_Representations_WACV_2020_paper.html">
                <papertitle>Animating Face using Disentangled Audio Representations</papertitle>
              </a>
              <br>
              <strong>Gaurav Mittal</strong>,
              <a href="https://sites.google.com/site/zjuwby/">Baoyuan Wang</a>
              <br>
              <em>WACV</em>, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Mittal_Animating_Face_using_Disentangled_Audio_Representations_WACV_2020_paper.pdf">paper</a> /
              <a href="https://www.youtube.com/watch?v=EbUOg1gVcFw&t=1092s">video</a> /
              <a href="https://arxiv.org/abs/1910.00726">arXiv</a>
              <p></p>
              <p>
                To make talking head generation robust to such emotional and noise variations, we propose an explicit audio representation learning framework that disentangles audio sequences into various factors such as phonetic content, emotional tone, background noise and others. When conditioned on disentangled content representation, the generated mouth movement by our model is significantly more accurate than previous approaches (without disentangled learning) in the presence of noise and emotional variations.
              </p>
            </td>
          </tr>

          <tr onmouseout="nerfie_stop()" onmouseover="nerfie_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='nerfie_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerfie_after.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/iclr_deepgen.png' width="160">
              </div>
              <!-- <script type="text/javascript">
                function nerfie_start() {
                  document.getElementById('nerfie_image').style.opacity = "1";
                }
                function nerfie_stop() {
                  document.getElementById('nerfie_image').style.opacity = "0";
                }
                nerfie_stop()
              </script> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/forum?id=SJx-SULKOV">
                <papertitle>Interactive Image Generation Using Scene Graphs</papertitle>
              </a>
              <br>

              <strong>Gaurav Mittal*</strong>,
              <a href="https://kbyagnik.github.io/">Shubham Agrawal*</a>,
              Anuva Agarwal*,
              Sushant Mehta*,
              <a href="https://tm157.github.io/">Tanya Marwah*</a> (*Equal Contribution)
              <br>
              <em>ICLR</em>, 2019 <a href="https://deep-gen-struct.github.io/index.html">DeepGenStruct Workshop</a>
              <br>
              <a href="https://openreview.net/pdf?id=SJx-SULKOV">paper</a> /
              <a href="https://arxiv.org/abs/1905.03743">arXiv</a>
              <p></p>
              <p>Proposed a method to generate an image incrementally based on a sequence of scene graphs such that the image content generated in previous steps is preserved and the cumulative image is modified as per the newly provided scene information.
              </p>
            </td>
          </tr>


          <tr onmouseout="nerv_stop()" onmouseover="nerv_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='nerv_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/hotdog.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/iccv_attentive.gif' width="160">
              </div>
              <!-- <script type="text/javascript">
                function nerv_start() {
                  document.getElementById('nerv_image').style.opacity = "1";
                }

                function nerv_stop() {
                  document.getElementById('nerv_image').style.opacity = "0";
                }
                nerv_stop()
              </script> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content_iccv_2017/html/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.html">
                <papertitle>Attentive Semantic Video Generation Using Captions</papertitle>
              </a>
              <br>
              <a href="https://tm157.github.io/">Tanya Marwah*</a>,
              <strong>Gaurav Mittal*</strong>,
              <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a> (* Equal Contribution)
              <br>
              <em>ICCV</em>, 2017
              <br>
              <a href="https://openaccess.thecvf.com/content_iccv_2017/html/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.html">paper</a> /
              <a href="https://github.com/Singularity42/cap2vid">code</a> /
              <a href="https://arxiv.org/abs/1708.05980">arXiv</a>
              <p></p>
              <p>Proposed a network architecture that learns long-term and short-term context of the video data and uses attention to align the information with accompanying text to perform variable length semantic video generation on unseen caption combinations.</p>
            </td>
          </tr>


          <tr onmouseout="winr_stop()" onmouseover="winr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='winr_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/notre_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/acmm_sync_draw.gif' width="160">
              </div>
              <!-- <script type="text/javascript">
                function winr_start() {
                  document.getElementById('winr_image').style.opacity = "1";
                }
                function winr_stop() {
                  document.getElementById('winr_image').style.opacity = "0";
                }
                winr_stop()
              </script> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/3123266.3123309">
                <papertitle>Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures</papertitle>
              </a>
              <br>
              <strong>Gaurav Mittal*</strong>,
              <a href="https://tm157.github.io/">Tanya Marwah*</a>,
              <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a> (* Equal Contribution)
              <br>
              <em>ACM Multimedia</em>, 2017 <font color="red"><strong>(Oral Presentation, 7.5% acceptance rate)</strong></font>
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3123266.3123309">paper</a> /
              <a href="https://arxiv.org/abs/1611.10314">arXiv</a>
              <p></p>
              <p>Combines a variational autoencoder (VAE) with recurrent attention mechanism to create a temporally dependent sequence of frames that are gradually formed over time.</p>
            </td>
          </tr>

          <tr onmouseout="nerd_stop()" onmouseover="nerd_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='nerd_image'><video  width=100% height=100% muted autoplay loop>
                <source src="images/nerd_160.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div> -->
                <img src='images/spot_garbage.png' width="160">
              </div>
              <!-- <script type="text/javascript">
                function nerd_start() {
                  document.getElementById('nerd_image').style.opacity = "1";
                }

                function nerd_stop() {
                  document.getElementById('nerd_image').style.opacity = "0";
                }
                nerd_stop()
              </script> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.youtube.com/watch?v=cTkCsz5C8zs&ab_channel=KudaPehchano">
                <papertitle>SpotGarbage: Smartphone App to Detect Garbage using Deep Learning</papertitle>
              </a>
              <br>
              <strong>Gaurav Mittal</strong>,
              <a href="https://kbyagnik.github.io/">Kaushal B Yagnik</a>,
              Mohit Garg,
              <a href="https://cse.iitrpr.ac.in/ckn/people/ckn.html">Narayanan C Krishnan</a>
              <br>
              <em>ACM UbiComp</em>, 2016
              <br>
              <!-- <a href="https://markboss.me/publication/2021-nerd/">project page</a> / -->
              <a href="https://www.youtube.com/watch?v=cTkCsz5C8zs&ab_channel=KudaPehchano">video</a> /
              <a href="https://github.com/KudaP/SpotGarbage">code</a> /
              <a href="https://dl.acm.org/doi/abs/10.1145/2971648.2971731">paper</a>/
              <a href="https://github.com/spotgarbage/spotgarbage-GINI">dataset</a>
              <p></p>
              <p>
              Designed a fully convolutional network to detect and coarsely segment garbage regions in the image. Built an smartphone app, SpotGarbage, deploying the CNN to make on-the-device detections. Also introduced a new Garbage-In-Images (GINI) dataset.
              </p>
            </td>
          </tr>


          <tr onmouseout="lssr_stop()" onmouseover="lssr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lssr_image'>
                  <img src='images/icvgip_brain.png' width="160"></div>
                <img src='images/icvgip_brain.png' width="160">
              </div>
              <!-- <script type="text/javascript">
                function lssr_start() {
                  document.getElementById('lssr_image').style.opacity = "1";
                }

                function lssr_stop() {
                  document.getElementById('lssr_image').style.opacity = "0";
                }
                lssr_stop()
              </script> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dl.acm.org/doi/10.1145/3009977.3010016">
                <papertitle>Supervised deep segmentation network for brain extraction</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.co.in/citations?user=_XL5F5kAAAAJ&hl=en">Apoorva Sikka*</a>,
              <strong>Gaurav Mittal*</strong>,
              <a href="https://www.iitrpr.ac.in/cse/bathula">Deepthi R Bathula</a>,
              <a href="https://cse.iitrpr.ac.in/ckn/people/ckn.html">Narayanan C Krishnan</a> (*Equal Contribution)
              <br>
              <em>ICVGIP</em>, 2016
              <br>
              <a href="https://dl.acm.org/doi/10.1145/3009977.3010016">paper</a>
              <p></p>
              <p>
                Proposed a novel encode-decoder network for brain extraction from T1-weighted MR images. The model operates on full 3D volumes, simplifying pre- and post-processing operations, to efficiently provide a voxel-wise binary mask delineating the brain region.
              </p>
            </td>
          </tr>


        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <ul>
                <li> <b>Reviewer</b> at ICLR 2022, ECCV 2022, NeurIPS 2022, AAAI 2021 <br>
                <li> <b>Outstanding Reviewer</b> at ICCV 2021, CVPR 2021, CVPR 2022
                <li> <b>Program Committee member</b> of <a href="https://cvmi-workshop.github.io/index.html">IEEE Workshop on Computer Vision for Microscopy Image Analysis (CVMI)</a> held in conjunction with CVPR 2022, CVPR 2021, CVPR 2020
                <li> <b>Program Chair</b> of <a href="https://nasfw20.github.io/">Workshop on Neural Architecture Search for Computer Vision in the Wild (NASFW)</a> held in conjunction with WACV 2020
              <ul>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='lssr_image'>
                  <img src='images/nasfw20.jpeg' width="160"></div>
                <img src='images/nasfw20.jpeg' width="160">
              </div>
              <!-- <script type="text/javascript">
                function lssr_start() {
                  document.getElementById('lssr_image').style.opacity = "1";
                }

                function lssr_stop() {
                  document.getElementById('lssr_image').style.opacity = "0";
                }
                lssr_stop()
              </script> -->
            </td>
            <td width="75%" valign="center">
              <a href="https://nasfw20.github.io/">Workshop on Neural Architecture Search for Computer Vision in the Wild (NASFW)</a> <br>
              <a href="http://wacv20.wacv.net/"> WACV 2020 </a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Respectfully copied from <a href="https://jonbarron.info/">Jon Barron's</a>
                website.

              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
